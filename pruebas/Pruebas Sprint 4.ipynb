{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas Sprint 4\n",
    "En este notebook se concentran todas las pruebas relativas al código generado durante el sprint 3, para ejecutarlas es necesario colocar el conjunto de datos preprocesado en la carpeta (ticnn_preprocessed.csv) en la carpeta datasets del proyecto y el archivo nlp_functions.py en el directorio raíz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 11: Grid Search con un subset de los hiperparámetros\n",
    "Esta prueba puede parecer trivial en un principio, pero resulta de vital importancia para este Sprint. Realizar la optimización con todos las combinaciones de hiperparámetros es una tarea que consume muchos recursos temporales y computacionales. Por estas razones, resulta lógico realizar primero la optimización empleando solamente un subconjunto de estas combinaciones, para comprobar si funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Juan\n",
      "[nltk_data]     Carlos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as tf_tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Importamos el plugin HParams de Tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "import nlp_functions as nlp_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones necesarias\n",
    "Esta es una versión modificada de la función train_test_rnn, se diferencia de la original en que solamente tiene en cuenta el ratio de Dropout como hiperparámetro y en que solo se entrena durante una época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_rnn(hparams, train_padded, test_padded, y_train, y_test):\n",
    "    # ----Comenzamos definiendo el modelo\n",
    "    model = tf.keras.Sequential()\n",
    "    # Capa de Embedding\n",
    "    model.add(tf.keras.layers.Embedding(input_dim = 20000, # Tamaño del vocabulario\n",
    "                                       output_dim = 100, # Número de dimensiones de WE\n",
    "                                       embeddings_initializer = 'uniform',\n",
    "                                       mask_zero = True))\n",
    "    # Capa bidireccional\n",
    "    model.add(tf.keras.layers.Bidirectional(\n",
    "        # Capa LSTM\n",
    "        tf.keras.layers.LSTM(units = 128,   # Hiperparámetro a optimizar\n",
    "                            activation = 'tanh',\n",
    "                            recurrent_activation = 'sigmoid',\n",
    "                            use_bias = True,\n",
    "                            dropout = hparams['HP_DROPOUT'],   # Hiperparámetro a optimizar\n",
    "                            recurrent_dropout = 0.05)))\n",
    "    # Capa densa 1\n",
    "    model.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "    \n",
    "    # Capa Dropout 1\n",
    "    model.add(tf.keras.layers.Dropout(rate = hparams['HP_DROPOUT']))  # Hiperparámetro a optimizar\n",
    "    \n",
    "    # Capa densa 2\n",
    "    model.add(tf.keras.layers.Dense(units = 64, activation = 'relu'))\n",
    "    \n",
    "    # Capa Dropout 2\n",
    "    model.add(tf.keras.layers.Dropout(rate = hparams['HP_DROPOUT']))  # Hiperparámetro a optimizar\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "    \n",
    "    # Definimos el optimizador\n",
    "    rmsprop_optim = tf.keras.optimizers.RMSprop(learning_rate = 0.001)  # Hiperparámetro a optimizar\n",
    "    \n",
    "    # ----Compilamos el modelo\n",
    "    model.compile(optimizer= rmsprop_optim, loss = 'binary_crossentropy',\n",
    "                 metrics = ['accuracy', 'Precision', 'Recall', nlp_f.f1_score])\n",
    "    \n",
    "    # ----Entrenar el modelo (No es necesario llamar a tensorboard)\n",
    "    model.fit(train_padded, np.array(y_train), epochs = 1,\n",
    "             batch_size = 100)    # Hiperparámetro a optimizar\n",
    "    \n",
    "    # ----Evaluar el conjunto de pruebas\n",
    "    # loss, acc, pre, rec, f1\n",
    "    _, _, _, _, f1 = model.evaluate(test_padded, np.array(y_test))\n",
    "    \n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams, train_padded, test_padded, y_train, y_test):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)   # Almacenar los valores de los parámetros usados\n",
    "        f1 = train_test_rnn(hparams, train_padded, test_padded, y_train, y_test)  # Entrenamos el modelo con los valores especificados\n",
    "        \n",
    "        tf.summary.scalar('f1-score', f1, step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(input):\n",
    "  res = ''\n",
    "  for word in input:\n",
    "    res = res + ' ' + word \n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicio de la prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba11():\n",
    "    print('Prueba 11: Grid Search con un subset de los hiperparámetros')\n",
    "    \n",
    "    try:\n",
    "        print('\\n--- Lectura del dataset preprocesado')\n",
    "        df = pd.read_csv('../dataset/ticnn_preprocessed.csv')\n",
    "    except:\n",
    "        print('Se ha producido un error al leer el dataset')\n",
    "        raise\n",
    "        \n",
    "    print('Dataset Leído correctamente')\n",
    "    \n",
    "    try:\n",
    "        print('\\n--- División de los datos en entrenamiento y pruebas')\n",
    "        # Alteración del orden\n",
    "        df = df.sample(frac = 1.)\n",
    "        # Transformamos la columna text a lista de string\n",
    "        from ast import literal_eval\n",
    "        df['text'] = df['text'].apply(literal_eval)\n",
    "        \n",
    "        # Aplicamos la función convert_to_string a los textos\n",
    "        df['text'] = df['text'].apply(convert_to_string)\n",
    "        \n",
    "        # Convertimos los textos y las targets variables a listas\n",
    "        texts = list(df['text'])\n",
    "        targets = list(df['type'])\n",
    "        \n",
    "        # División en conjunto de entrenamiento y test\n",
    "        x_train, y_train, x_test, y_test = nlp_f.train_test_split(texts, targets)\n",
    "        \n",
    "    except:\n",
    "        print('Error al separar en entrenamiento y pruebas')\n",
    "        raise\n",
    "        \n",
    "    print('Datos separados correctamente')\n",
    "    \n",
    "    try:\n",
    "        print('\\n--- Creación del diccionario (Vocabulario)')\n",
    "        tokenizer = tf_tokenizer(num_words = 20000, oov_token = '<null_token>', \n",
    "                                 lower = False, char_level = False)\n",
    "        tokenizer.fit_on_texts(x_train)\n",
    "    \n",
    "    except:\n",
    "        print('Error al crear el vocabulario')\n",
    "        raise\n",
    "        \n",
    "    print('Vocabulario creado correctamente')\n",
    "    \n",
    "    try:\n",
    "        print('\\n--- Transformar los textos en secuencias y padding')\n",
    "        x_train_sequence = tokenizer.texts_to_sequences(x_train)\n",
    "        x_test_sequence = tokenizer.texts_to_sequences(x_test)\n",
    "        \n",
    "        train_padded = pad_sequences(x_train_sequence, maxlen = 600, \n",
    "                                     dtype = 'int32', truncating = 'post', \n",
    "                                     padding = 'post')\n",
    "        test_padded = pad_sequences(x_test_sequence, maxlen = 600, \n",
    "                                    dtype = 'int32', truncating = 'post', \n",
    "                                    padding = 'post')\n",
    "\n",
    "    except:\n",
    "        print('Error al convertir los textos en secuencias')\n",
    "        raise\n",
    "        \n",
    "    print('Secuenciación y padding realizados con éxito')\n",
    "    \n",
    "    try:\n",
    "        print('\\n--- Carga de TensorBoard y limpieza de logs previos')\n",
    "        # Cargamos la extensión de TB para notebooks\n",
    "        %load_ext tensorboard\n",
    "\n",
    "        # Limpiar logs de ejecuciones anteriores \n",
    "        !rm -rf ./logs/ \n",
    "    except:\n",
    "        print('Error al cargar TensorBoard')\n",
    "        raise\n",
    "    print('TensorBoard Cargado correctamente')\n",
    "    \n",
    "    try:\n",
    "        print('\\n--- Definición de los hiperparámetros a optimizar')\n",
    "        HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.2, 0.4))\n",
    "        METRIC_F1 = 'f1-score' \n",
    "\n",
    "        with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "            hp.hparams_config(\n",
    "            hparams = [HP_DROPOUT],\n",
    "            metrics = [hp.Metric(METRIC_F1, display_name = 'F1-Score')])\n",
    "    except:\n",
    "        print('Error al definir los hiperparámetros')\n",
    "        raise\n",
    "    print('Hiperparámetros definidos correctamente')\n",
    "    \n",
    "    try:\n",
    "        print('\\n--- Ejecución Grid Search')\n",
    "        session_num = 0\n",
    "\n",
    "        for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "            hparams = {'HP_DROPOUT': dropout_rate}\n",
    "                \n",
    "            run_name = 'run-%d' % session_num\n",
    "            print('--Iniciando ejecución : %s' % run_name)\n",
    "            print({h: hparams[h] for h in hparams})\n",
    "            run('logs/hparam_tuning/' + run_name, \n",
    "                hparams, train_padded, test_padded, y_train, y_test)\n",
    "            session_num += 1\n",
    "    except:\n",
    "        print('Error al realizar Grid Search')\n",
    "        raise\n",
    "    \n",
    "    print('\\n-------------------------------------------------------------------')\n",
    "    print('Grid Search realizado con éxito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba 11: Grid Search con un subset de los hiperparámetros\n",
      "\n",
      "--- Lectura del dataset preprocesado\n",
      "Dataset Leído correctamente\n",
      "\n",
      "--- División de los datos en entrenamiento y pruebas\n",
      "Datos separados correctamente\n",
      "\n",
      "--- Creación del diccionario (Vocabulario)\n",
      "Vocabulario creado correctamente\n",
      "\n",
      "--- Transformar los textos en secuencias y padding\n",
      "Secuenciación y padding realizados con éxito\n",
      "\n",
      "--- Carga de TensorBoard y limpieza de logs previos\n",
      "TensorBoard Cargado correctamente\n",
      "\n",
      "--- Definición de los hiperparámetros a optimizar\n",
      "Hiperparámetros definidos correctamente\n",
      "\n",
      "--- Ejecución Grid Search\n",
      "--Iniciando ejecución : run-0\n",
      "{'HP_DROPOUT': 0.2}\n",
      "Train on 13596 samples\n",
      "13596/13596 [==============================] - 564s 41ms/sample - loss: 0.3003 - accuracy: 0.8901 - Precision: 0.8824 - Recall: 0.9382 - f1_score: 0.9109\n",
      "5827/5827 [==============================] - 70s 12ms/sample - loss: 0.1333 - accuracy: 0.9475 - Precision: 0.9530 - Recall: 0.9558 - f1_score: 0.9529\n",
      "--Iniciando ejecución : run-1\n",
      "{'HP_DROPOUT': 0.4}\n",
      "Train on 13596 samples\n",
      "13596/13596 [==============================] - 564s 41ms/sample - loss: 0.3292 - accuracy: 0.8800 - Precision: 0.8639 - Recall: 0.9449 - f1_score: 0.9072\n",
      "5827/5827 [==============================] - 70s 12ms/sample - loss: 0.1332 - accuracy: 0.9490 - Precision: 0.9575 - Recall: 0.9537 - f1_score: 0.9540\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Grid Search realizado con éxito\n"
     ]
    }
   ],
   "source": [
    "prueba11()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
