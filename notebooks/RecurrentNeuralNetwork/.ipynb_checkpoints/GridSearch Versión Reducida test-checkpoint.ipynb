{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search (Optimización de Hiperparámetros) (versión reducida)\n",
    "A la hora de crear modelos en el mundo del Machine Learning, es necesario seleccionar varios hiperparámetros, como el ratio de Dropout y el learning rate. La elección de los valores de estos hiperparámetros tienen repercusiones directas en las métricas que evaluan el rendimiento del modelo, como f1-score. Por lo tanto, un paso imprescindible en el proceso de entrenamiento de un modelo consiste en identificar los mejores hiperparámetros para el problema, que deben obtenerse mediante experimentación. Este proceso se conoce como Optimización de Hiperparámetros o, en inglés, \"Hyperparameter Tuning\". <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Juan\n",
      "[nltk_data]     Carlos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as tf_tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "import nlp_functions as nlp_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset en memoria\n",
    "df = pd.read_csv('../../dataset/ticnn_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['donald', 'trump', 'property', 'showcase', 'b...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['trump', 'foundation', 'tell', 'new', 'york',...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['donald', 'trump', 'prepares', 'white', 'hous...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['lure', 'chinese', 'investor', 'trump', 'name...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['melania', 'barron', 'trump', 'wont', 'immedi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  type\n",
       "0  ['donald', 'trump', 'property', 'showcase', 'b...   0.0\n",
       "1  ['trump', 'foundation', 'tell', 'new', 'york',...   0.0\n",
       "2  ['donald', 'trump', 'prepares', 'white', 'hous...   0.0\n",
       "3  ['lure', 'chinese', 'investor', 'trump', 'name...   0.0\n",
       "4  ['melania', 'barron', 'trump', 'wont', 'immedi...   0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alteración del orden\n",
    "df = df.sample(frac = 1.)\n",
    "\n",
    "# Transformamos la columna text a lista de string\n",
    "from ast import literal_eval\n",
    "df['text'] = df['text'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9749</th>\n",
       "      <td>[egon, von, greyerz, broadcast, interview, ava...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14612</th>\n",
       "      <td>[w, well, educate, graduate, two, ivy, league,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>[star, general, snag, lie, stuxnet, leak, prob...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>[election, medias, movie, reality, fail, jon, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16526</th>\n",
       "      <td>[obama, seek, calm, fear, terrorism, ahead, ho...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  type\n",
       "9749   [egon, von, greyerz, broadcast, interview, ava...   1.0\n",
       "14612  [w, well, educate, graduate, two, ivy, league,...   1.0\n",
       "4869   [star, general, snag, lie, stuxnet, leak, prob...   0.0\n",
       "5226   [election, medias, movie, reality, fail, jon, ...   1.0\n",
       "16526  [obama, seek, calm, fear, terrorism, ahead, ho...   0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(input):\n",
    "  res = ''\n",
    "  for word in input:\n",
    "    res = res + ' ' + word \n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la función convert_to_string a los textos\n",
    "df['text'] = df['text'].apply(convert_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9749</th>\n",
       "      <td>egon von greyerz broadcast interview availabl...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14612</th>\n",
       "      <td>w well educate graduate two ivy league school...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>star general snag lie stuxnet leak probe poli...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  type\n",
       "9749    egon von greyerz broadcast interview availabl...   1.0\n",
       "14612   w well educate graduate two ivy league school...   1.0\n",
       "4869    star general snag lie stuxnet leak probe poli...   0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos los textos y las targets variables a listas\n",
    "texts = list(df['text'])\n",
    "targets = list(df['type'])\n",
    "\n",
    "# División en conjunto de entrenamiento y test\n",
    "x_train, y_train, x_test, y_test = nlp_f.train_test_split(texts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el diccionario (vocabulario)\n",
    "tokenizer = tf_tokenizer(num_words = 20000, oov_token = '<null_token>', lower = False, char_level = False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "word_dict = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos los textos en secuencias\n",
    "x_train_sequence = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_sequence = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paddeamos las secuencias\n",
    "train_padded = pad_sequences(x_train_sequence, maxlen = 600, dtype = 'int32', truncating = 'post', padding = 'post')\n",
    "test_padded = pad_sequences(x_test_sequence, maxlen = 600, dtype = 'int32', truncating = 'post', padding = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cargamos la extensión de TB para notebooks\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Limpiar logs de ejecuciones anteriores \n",
    "# (necesario ejecutar conda install posix en Anaconda prompt para poder ejecutarlo en Windows 10)\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de los hiperparámetros \n",
    "Listamos los hiperparámetros a optimizar y sus valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el plugin HParams de Tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# Listamos los parámetros a optimizar y sus distintos valores\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.2, 0.4))\n",
    "# HP_LR = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.05]))\n",
    "# HP_BATCH = hp.HParam('batch_size', hp.Discrete([50, 100, 200]))\n",
    "# HP_LSTM = hp.HParam('lstm_units', hp.Discrete([128, 256]))\n",
    "\n",
    "METRIC_F1 = 'f1-score'    #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams = [HP_DROPOUT\n",
    "#                , HP_LR, HP_BATCH, HP_LSTM\n",
    "              ],\n",
    "    metrics = [hp.Metric(METRIC_F1, display_name = 'F1-Score')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo\n",
    "Definimos una función que entrene a la red neuronal recurrente definida durante el sprint 3 con unos parámetros determinados, el modelo devolverá el valor de f1-score en el conjunto de test una vez finalizado el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_rnn(hparams):\n",
    "    # ----Comenzamos definiendo el modelo\n",
    "    model = tf.keras.Sequential()\n",
    "    # Capa de Embedding\n",
    "    model.add(tf.keras.layers.Embedding(input_dim = 20000, # Tamaño del vocabulario\n",
    "                                       output_dim = 100, # Número de dimensiones de WE\n",
    "                                       embeddings_initializer = 'uniform',\n",
    "                                       mask_zero = True))\n",
    "    # Capa bidireccional\n",
    "    model.add(tf.keras.layers.Bidirectional(\n",
    "        # Capa LSTM\n",
    "        tf.keras.layers.LSTM(units = 128,   # Hiperparámetro a optimizar\n",
    "                            activation = 'tanh',\n",
    "                            recurrent_activation = 'sigmoid',\n",
    "                            use_bias = True,\n",
    "                            dropout = hparams[HP_DROPOUT],   # Hiperparámetro a optimizar\n",
    "                            recurrent_dropout = 0.05)))\n",
    "    # Capa densa 1\n",
    "    model.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "    \n",
    "    # Capa Dropout 1\n",
    "    model.add(tf.keras.layers.Dropout(rate = hparams[HP_DROPOUT]))  # Hiperparámetro a optimizar\n",
    "    \n",
    "    # Capa densa 2\n",
    "    model.add(tf.keras.layers.Dense(units = 64, activation = 'relu'))\n",
    "    \n",
    "    # Capa Dropout 2\n",
    "    model.add(tf.keras.layers.Dropout(rate = hparams[HP_DROPOUT]))  # Hiperparámetro a optimizar\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "    \n",
    "    # Definimos el optimizador\n",
    "    rmsprop_optim = tf.keras.optimizers.RMSprop(learning_rate = 0.001)  # Hiperparámetro a optimizar\n",
    "    \n",
    "    # ----Compilamos el modelo\n",
    "    model.compile(optimizer= rmsprop_optim, loss = 'binary_crossentropy',\n",
    "                 metrics = ['accuracy', 'Precision', 'Recall', nlp_f.f1_score])\n",
    "    \n",
    "    # ----Entrenar el modelo (No es necesario llamar a tensorboard)\n",
    "    model.fit(train_padded, np.array(y_train), epochs = 1,\n",
    "             batch_size = 100)    # Hiperparámetro a optimizar\n",
    "    \n",
    "    # ----Evaluar el conjunto de pruebas\n",
    "    # loss, acc, pre, rec, f1\n",
    "    _, _, _, _, f1 = model.evaluate(test_padded, np.array(y_test))\n",
    "    \n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las ejecuciones, para cada ejecución guardamos los hiperparámetros utilizados y el resultado obtenido en el log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)   # Almacenar los valores de los parámetros usados\n",
    "        f1 = train_test_rnn(hparams)  # Entrenamos el modelo con los valores especificados\n",
    "        \n",
    "        tf.summary.scalar(METRIC_F1, f1, step = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecuciones\n",
    "Realizamos el entrenamiento con todos los valores de los hiperparámetros y guardamos los logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Iniciando ejecución : run-0\n",
      "{'dropout': 0.2}\n",
      "Train on 13596 samples\n",
      "13596/13596 [==============================] - 562s 41ms/sample - loss: 0.3345 - accuracy: 0.8839 - Precision: 0.8714 - Recall: 0.9416 - f1_score: 0.9091\n",
      "5827/5827 [==============================] - 70s 12ms/sample - loss: 0.1472 - accuracy: 0.9434 - Precision: 0.9465 - Recall: 0.9555 - f1_score: 0.9441\n",
      "--Iniciando ejecución : run-1\n",
      "{'dropout': 0.4}\n",
      "Train on 13596 samples\n",
      "13596/13596 [==============================] - 564s 41ms/sample - loss: 0.3061 - accuracy: 0.8803 - Precision: 0.8690 - Recall: 0.9380 - f1_score: 0.9064\n",
      "5827/5827 [==============================] - 71s 12ms/sample - loss: 0.1398 - accuracy: 0.9449 - Precision: 0.9459 - Recall: 0.9591 - f1_score: 0.9455\n",
      "Wall time: 21min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_num = 0\n",
    "\n",
    "#     for learning_rate in HP_LR.domain.values:\n",
    "#         for batch_size in HP_BATCH.domain.values:\n",
    "#             for lstm_u in HP_LSTM.domain.values:\n",
    "\n",
    "for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "    hparams = {HP_DROPOUT: dropout_rate}\n",
    "                \n",
    "    run_name = 'run-%d' % session_num\n",
    "    print('--Iniciando ejecución : %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    run('logs/hparam_tuning/' + run_name, hparams)\n",
    "    session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los resultados en TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
