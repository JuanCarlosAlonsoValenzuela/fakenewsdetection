{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Técnicas de preprocesado empleadas:\n",
    "1. **Lowercase:** Convertimos todo el texto a minúsculas para evitar repeticiones de palabras, esto puede ocurrir cuando ciertas funciones no son case sensitive\n",
    "2. **Eliminar etiquetas html, direcciones email y urls:** Para hacer esto se han empleado expresiones regulares\n",
    "3. **Eliminar signos de puntuación**\n",
    "4. **POS Tagging:** Asignar una categoría a cada palabra (ejemplo: adjetivo, nombre, adverbio, verbo...). Esto es necesario para realizar Lemmatization con éxito.\n",
    "5. **Lemmatization:** Recibe el tipo de palabra (obtenido en POS Tagging) como parámetro, en caso contrario considera a la palabra como un nombre, lo que afecta considerablemente a la utilidad de esta función.\n",
    "6. **Stop Words Removal:** Elimina palabras innecesarias que generalmente hacen que los algoritmos tengan peor rendimiento. Ejemplos: Phrasal verbs, preposiciones, entre otros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función POS-tagging devuelve más valores de los que se necesitan, mientras que la función de lematización solamente necesita nombre, adjetivo, adverbio y nombre. Para convertir de uno a otro usamos un map/diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_map = {\n",
    "'CC': 'n','CD': 'n', 'DT': 'n','EX': 'n', 'FW': 'n','IN': 'n', 'JJ': 'a','JJR': 'a', 'JJS': 'a','LS': 'n', 'MD': 'v','NN': 'n',\n",
    "'NNS': 'n','NNP': 'n', 'NNPS': 'n','PDT': 'n', 'POS': 'n','PRP': 'n', 'PRP$': 'r','RB': 'r', 'RBR': 'r','RBS': 'r', 'RP': 'n','TO': 'n',\n",
    "'UH': 'n','VB': 'v', 'VBD': 'v','VBG': 'v', 'VBN': 'v','VBP': 'v', 'VBZ': 'v','WDT': 'n', 'WP': 'n','WP$': 'n', 'WRB': 'r'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las stop words del inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En inglés hay un total de  179 stopwords\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print('En inglés hay un total de ', len(stop_words), 'stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos las primeras 10 stopwords como ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['now',\n",
       " 'why',\n",
       " \"isn't\",\n",
       " 'further',\n",
       " 'once',\n",
       " 'his',\n",
       " 'about',\n",
       " 'more',\n",
       " 'be',\n",
       " 'what']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stop_words)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase inicial:  <HTML>This <p>is.a</p> ! jua@email.com sentences, showing off the <br> stop words filtration. http://www.youtube.com \n",
      "\n",
      "Frase sin direcciones de email, etiquetas html y urls this is.a !   sentences, showing off the  stop words filtration. \n",
      "Palabras con sus tags asignados\n",
      "tags [('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentences', 'NNS'), ('showing', 'VBG'), ('off', 'RP'), ('the', 'DT'), ('stop', 'NN'), ('words', 'NNS'), ('filtration', 'NN')]\n",
      "Frase tras aplicar preprocesado: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sentence', 'show', 'stop', 'word', 'filtration']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "example_sent = \"<HTML>This <p>is.a</p> ! jua@email.com sentences, showing off the <br> stop words filtration. http://www.youtube.com\"\n",
    "# Convertimos a minúscula (necesario para stopwords)\n",
    "print('Frase inicial: ', example_sent, '\\n')\n",
    "example_sent = example_sent.lower()\n",
    "\n",
    "\n",
    "# Eliminar etiquetas html, direcciones email y urls \n",
    "# HTML TAGS (al ser resultado de web scrapping, es conveniente asegurarse)\n",
    "from bs4 import BeautifulSoup\n",
    "example_sent = BeautifulSoup(example_sent, 'lxml').text\n",
    "\n",
    "# EMAIL ADDRESSES\n",
    "import re\n",
    "example_sent = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', example_sent)\n",
    "\n",
    "# URLs\n",
    "example_sent = re.sub(r'http\\S+', '', example_sent)\n",
    "\n",
    "# Eliminar signos de puntuación\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "word_tokens = tokenizer.tokenize(example_sent)\n",
    "# Ahora tenemos las palabras tokenizadas sin signos de puntuación y con stop words\n",
    "print('Frase sin direcciones de email, etiquetas html y urls', example_sent)\n",
    "\n",
    "# Realizamos POS Tagging (eliminaremos las stopwords más adelante, ya que estas mejoran la precisión de POS tagging)\n",
    "# Este método devuelve una lista de tuplas del tipo (palabra, categoría)\n",
    "tags = nltk.pos_tag(word_tokens)\n",
    "print('Palabras con sus tags asignados')\n",
    "print('tags', tags)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Aplicamos lematización a todas las palabras del texto según su categoría\n",
    "for i, word in enumerate(word_tokens):\n",
    "    # Si la key no aparece en el mapa, se le considera un nombre, de esta forma se evitan errores de compilación\n",
    "    word_tokens[i] = lemmatizer.lemmatize(word, pos=pos_map.get(tags[i][1] , 'n'))\n",
    "\n",
    "# Quitamos las stop words\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print('Frase tras aplicar preprocesado: ')\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores que devuelve POS Tagging\n",
    "\n",
    "- **CC:**\tcoordinating conjunction\n",
    "- **CD:**\tcardinal digit\n",
    "- **DT:**\tdeterminer\n",
    "- **EX:**\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "- **FW:**\tforeign word\n",
    "- **IN:**\tpreposition/subordinating conjunction\n",
    "- **JJ:**\tadjective\t'big'\n",
    "- **JJR:**\tadjective, comparative\t'bigger'\n",
    "- **JJS:**\tadjective, superlative\t'biggest'\n",
    "- **LS:**\tlist marker\t1)\n",
    "- **MD:**\tmodal\tcould, will\n",
    "- **NN:**\tnoun, singular 'desk'\n",
    "- **NNS:**\tnoun plural\t'desks'\n",
    "- **NNP:**\tproper noun, singular\t'Harrison'\n",
    "- **NNPS:**\tproper noun, plural\t'Americans'\n",
    "- **PDT:**\tpredeterminer\t'all the kids'\n",
    "- **POS:** possessive ending\tparent\\'s\n",
    "- **PRP:**\tpersonal pronoun\tI, he, she\n",
    "- **PRP\\$:**\tpossessive pronoun\tmy, his, hers\n",
    "- **RB:**\tadverb\tvery, silently,\n",
    "- **RBR:**\tadverb, comparative\tbetter\n",
    "- **RBS:**\tadverb, superlative\tbest\n",
    "- **RP:**\tparticle\tgive up\n",
    "- **TO:**\tto\tgo 'to' the store.\n",
    "- **UH:**\tinterjection\terrrrrrrrm\n",
    "- **VB:**\tverb, base form\ttake\n",
    "- **VBD:**\tverb, past tense\ttook\n",
    "- **VBG:**\tverb, gerund/present participle\ttaking\n",
    "- **VBN:**\tverb, past participle\ttaken\n",
    "- **VBP:**\tverb, sing. present, non-3d\ttake\n",
    "- **VBZ:**\tverb, 3rd person sing. present\ttakes\n",
    "- **WDT:**\twh-determiner\twhich\n",
    "- **WP:**\twh-pronoun\twho, what\n",
    "- **WP\\$:**\tpossessive wh-pronoun\twhose\n",
    "- **WRB:**\twh-abverb\twhere, when\n",
    "\n",
    "### Categorías que recibe Lemmatization\n",
    "ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir el preprocesado en una función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenizer(example_sent):\n",
    "\n",
    "    example_sent = example_sent.lower()\n",
    "    \n",
    "    # HTML TAGS\n",
    "    example_sent = BeautifulSoup(example_sent, 'lxml').text\n",
    "\n",
    "    # EMAIL ADDRESSES\n",
    "    example_sent = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', example_sent)\n",
    "\n",
    "    # URLs\n",
    "    example_sent = re.sub(r'http\\S+', '', example_sent)\n",
    "\n",
    "    # Signos de puntuación\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_tokens = tokenizer.tokenize(example_sent)\n",
    "\n",
    "    # POS Tagging \n",
    "    tags = nltk.pos_tag(word_tokens)\n",
    "\n",
    "    # Lemmatization\n",
    "    for i, word in enumerate(word_tokens):\n",
    "        word_tokens[i] = lemmatizer.lemmatize(word, pos=pos_map.get(tags[i][1] , 'n'))\n",
    "\n",
    "    # stop words\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence', 'show', 'stop', 'word', 'filtration']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"<HTML>This <p>is.a</p> ! jua@email.com sentences, showing off the <br> stop words filtration. http://www.youtube.com\"\n",
    "tokenizer(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
